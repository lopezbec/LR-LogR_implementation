{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Multiple_Regularize_LRandLogR_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zr5d-t8sX4bD"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvCaYYygG0ak"
      },
      "source": [
        "<center><b>©Content is made available under the CC-BY-NC-ND 4.0 license. Christian Lopez, lopezbec@lafayette.edu<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWckiY_RphgE"
      },
      "source": [
        "#Multivariate and Regularized Linear Regression & Logistic Regression Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-kbu-zojyb"
      },
      "source": [
        "Most of the notebooks we are going to be using are inspired from existing notebooks that are available online and are made  free for educational purposes. Nonetheless, the notebooks of this class should not be share without prior permission of the instructor. When working in an assignment always remember the [Student Code of Conduct]( https://conduct.lafayette.edu/student-handbook/student-code-of-conduct/).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Ms9NtuUHwi"
      },
      "source": [
        "###**Instructions:**\n",
        "- Only modify the code that is within the comments:\n",
        "\n",
        "`### START CODE HERE ###`\n",
        "\n",
        "`### END CODE HERE ###`\n",
        "\n",
        "- You need to run all the code cells on the notebok sequentially\n",
        "- If you are asked to change/update a cell, change/update and run it to check if your result is correct."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sHmZKgYUh1M"
      },
      "source": [
        "###Housekeeping Notes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86SZCRpyLAM"
      },
      "source": [
        "\n",
        "Before you  start working with this notebook, you need to first clone the repo with the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfET_vEVt4uw"
      },
      "source": [
        "!git clone https://github.com/lopezbec/LR-LogR_implementation\n",
        "%cd LR-LogR_implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QwasjN6GMgL"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFsdePPeGMgk"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "import os\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "#Scikit-learn for implemeting LinearRegression from a existing algorithm.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def computeCost(X, y, theta):\n",
        "    return 1/(2*y.size)*np.sum(np.square(X.dot(theta)-y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTmW_k3XaWDa"
      },
      "source": [
        "# 1- Linear regression with multiple variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aqzSLHRSnG"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "In this part, you will implement linear regression with multiple variables to predict the prices of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
        "\n",
        "The file `data/House_data_multiv.txt` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price\n",
        "of the house. \n",
        "\n",
        "<a id=\"section4\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTJwr7N2RSnI"
      },
      "source": [
        "# Load data\n",
        "data = np.loadtxt(os.path.join('data', 'House_data_multiv.txt'), delimiter=',')\n",
        "X = data[:, :2]\n",
        "y = data[:, 2]\n",
        "y=y[:,np.newaxis]\n",
        "\n",
        "m = y.size\n",
        "# print out some data points\n",
        "print('{:>8s}{:>8s}{:>10s}'.format('X[:,0]', 'X[:, 1]', 'y'))\n",
        "print('-'*26)\n",
        "for i in range(10):\n",
        "    print('{:8.0f}{:8.0f}{:10.0f}'.format(X[i, 0], X[i, 1], y[i,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pGfrvnCaU51"
      },
      "source": [
        "### 1.1- Feature Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhIGoitRaArf"
      },
      "source": [
        "\n",
        "\n",
        "We start by loading and displaying some values from this dataset. By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICCJlDtRSnN"
      },
      "source": [
        "Your task here is to complete the code in `feature_Normalize_implementation` function:\n",
        "- Subtract the mean value of each feature from the dataset.\n",
        "- After subtracting the mean, additionally scale (divide) the feature values by their respective “standard deviations.”\n",
        "\n",
        "The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within ±2 standard deviations of the mean); this is an alternative to taking the range of values (max-min). In `numpy`, you can use the `std` function to compute the standard deviation. \n",
        "\n",
        "For example, the quantity `X[:, 0]` contains all the values of $x_1$ (house sizes) in the training set, so `np.std(X[:, 0])` computes the standard deviation of the house sizes.\n",
        "At the time that the function `feature_Normalize_implementation` is called, the extra column of 1’s corresponding to $x_0 = 1$ has not yet been added to $X$. \n",
        "\n",
        "You will do this for all the features and your code should work with datasets of all sizes (any number of features / examples). Note that each column of the matrix $X$ corresponds to one feature.\n",
        "\n",
        "**Implementation Note:** When normalizing the features, it is important\n",
        "to store the values used for normalization - the mean value and the standard deviation used for the computations. After learning the parameters\n",
        "from the model, we often want to predict the prices of houses we have not\n",
        "seen before. Given a new x value (living room area and number of bedrooms), we must first normalize x using the mean and standard deviation that we had previously computed from the training set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZh6B10lakc-"
      },
      "source": [
        "#### Excersice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAEuXUM5RSnQ"
      },
      "source": [
        "def  feature_Normalize_implementation(X):\n",
        "    \"\"\"\n",
        "    Normalizes the features in X. returns a normalized version of X where\n",
        "    the mean value of each feature is 0 and the standard deviation\n",
        "    is 1. This is often a good preprocessing step to do when working with\n",
        "    learning algorithms.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        The dataset of shape (m x n).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    X_norm : array_like\n",
        "        The normalized dataset of shape (m x n).\n",
        "    \"\"\"\n",
        "    # You need to set these values correctly\n",
        "    X_norm = X.copy()\n",
        "    mu = np.zeros(X.shape[1])\n",
        "    sigma = np.zeros(X.shape[1])\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_norm, mu, sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRuSXTXJeow9"
      },
      "source": [
        "Lets check your code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cvW4XeWeogy"
      },
      "source": [
        "X_norm, mu, sigma= feature_Normalize_implementation(X)\n",
        "\n",
        "print('{:>8s}{:>15s}'.format('X_norm[:,0]', 'X_norm[:, 1]'))\n",
        "print('-'*26)\n",
        "for i in range(5):\n",
        "    print('{:>8.3f}{:>15.3f}'.format(X_norm[i, 0], X_norm[i, 1]))\n",
        "\n",
        "print(\"Features means:{:>9.3f}{:>14.3f}\".format(mu[0], mu[1]))\n",
        "print(\"Features Std:{:>10.3f}{:>15.3f}\".format(sigma[0], sigma[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ug-OPE37wP2"
      },
      "source": [
        "We can alos normalized our data using the `StandardScaler()` from  [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXFt77Pp7wll"
      },
      "source": [
        "### START CODE HERE ### (≈ 4 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "print('{:>8s}{:>15s}'.format('X_norm_sklearn[:,0]', 'X_norm_sklearn[:, 1]'))\n",
        "print('-'*26)\n",
        "for i in range(5):\n",
        "    print('{:>8.3f}{:>15.3f}'.format(X_norm_sklearn[i, 0], X_norm_sklearn[i, 1]))\n",
        "\n",
        "print(\"Features means:{:>9.3f}{:>13.3f}\".format(mu_sklearn[0],mu_sklearn[1]))\n",
        "print(\"Features Std:{:>10.3f}{:>14.3f}\".format(sigma_sklearn[0], sigma_sklearn[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4rROBq77HlT"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "\n",
        "```\n",
        "X_norm[:,0]   X_norm[:, 1]\n",
        "--------------------------\n",
        "   0.131         -0.226\n",
        "  -0.510         -0.226\n",
        "   0.508         -0.226\n",
        "  -0.744         -1.554\n",
        "   1.271          1.102\n",
        "Features means: 2000.681        3.170\n",
        "Features Std:   786.203         0.753\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFP9Prfp6qMl"
      },
      "source": [
        "**Sanity check:**\n",
        "\n",
        "If we get the mean and std of the normalized features we should have means approx. 1, stds approx. 0. If you look at the actual values without the rounding u (i.e., no `{:>12.3f}`) you will see it is not exactly 1 nor 0 due to computational error)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWx06UMd6rvH"
      },
      "source": [
        "X_norm2, mu2, sigma2= feature_Normalize_implementation(X_norm)\n",
        "print(\"Normalized features means:{:>8.3f}{:>12.3f}\".format(mu2[0], mu2[1]))\n",
        "print(\"Normalized features Std:{:>9.3f}{:>13.3f}\".format(sigma2[0], sigma2[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scFQvKE27Umi"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "Normalized features means:  -0.000       0.000\n",
        "Normalized features Std:    1.000        1.000\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npkyArm7Ys5Y"
      },
      "source": [
        "### 1.2 - Ridge Regression with scikit-learn \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz7DUgS0YsKe"
      },
      "source": [
        "\n",
        "The closed-form solution to Ridge Linear Regression is:\n",
        "\n",
        "$$ \\theta = \\left( X^T X + \\lambda A\\right)^{-1} X^T{y}$$\n",
        "\n",
        "Where $A$ is the $(n +1)\\times(n+1)$ identity matrix, except with a 0 in the top-left cell, corresponting tot he bias/intercept term. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tmM4yy134SY"
      },
      "source": [
        "#Let generate some data\n",
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5F5j5uW3hpW"
      },
      "source": [
        "#Create polynomial features\n",
        "Degree_of_the_Polynomial_Model=5\n",
        "\n",
        "#\"include_bias=False\" since we dont want to normalized the intercep/bias term\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "#Scaling our data is key for Regularized LR to work well\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "#Get the Mean and SD used to normalize the data, so we can apply it to our testing\n",
        "std_scaler = StandardScaler()\n",
        "X_scale=std_scaler.fit(X_poly)\n",
        "\n",
        "#Scaling our data is key for Regularized LR to work\n",
        "X_poly=std_scaler.fit_transform(X_poly)\n",
        "\n",
        "#If you comment the line above you will see gradient descent will not work well (or at all)\n",
        "X_poly = np.hstack([np.ones(shape=(y.size,1)), X_poly])\n",
        "\n",
        "# initialize fitting parameters (n+1)\n",
        "theta= np.zeros(Degree_of_the_Polynomial_Model+1).reshape(Degree_of_the_Polynomial_Model+1,1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JiawtgOapnM"
      },
      "source": [
        "#### Excersice: Fit a Ridge Linear Regression using `sklearn.linear_model.Ridge`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXsxT3T9NMkN"
      },
      "source": [
        "#Import Ridge Linear Regression\n",
        "from sklearn.linear_model import Ridge\n",
        "lambda_term=0.02\n",
        "\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "#Lets get the thetas\n",
        "coe=ridge_reg.coef_\n",
        "interc=ridge_reg.intercept_\n",
        "coe=coe.reshape(coe.size,1)\n",
        "interc=interc.reshape(interc.size,1)\n",
        "theta_sklearn=np.vstack([interc,coe[1:, :]])\n",
        "print(theta_sklearn)\n",
        "theta_sklearn_cost=computeCost(X_poly, y, theta_sklearn)\n",
        "print(\"Cost value= {}\".format(round(theta_sklearn_cost,4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSx0zqnqYrMf"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "For Degrees=5, lambda_term=0.02\n",
        "```\n",
        "[[ 1.50467735]\n",
        " [ 1.02273468]\n",
        " [-1.59103516]\n",
        " [-0.06054524]\n",
        " [ 0.47300252]\n",
        " [ 0.60908347]]\n",
        "Cost value= 0.1431\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-28hwQfr-aAy"
      },
      "source": [
        "### 1.3. Let’s plot our Regularized Linear Regression model.\n",
        "\n",
        "Pay attention on how we need to use the original mean and std used to transform the training set, to now transform our inputs for our predictions (the x to create our line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMIQ_sOPaaVV"
      },
      "source": [
        "#Lets create some new data to create a line\n",
        "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
        "#Add some polynomial term to this data\n",
        "poly_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "X_new_poly = poly_features.fit_transform(X_new)\n",
        "\n",
        "#WE NEED TO SCALE OUR DATA BASED ON THE SCALE WE USE FOR TRAINING!!!!\n",
        "X_new_poly_sclae=X_scale.transform(X_new_poly)\n",
        "\n",
        "#Add Theta_0\n",
        "X_new_poly_sclae_0 = np.hstack([np.ones(shape=(X_new.size,1)), X_new_poly_sclae])\n",
        "\n",
        "#Lets cacuate our model (h, y_hat)\n",
        "y_newbig_theta_best= X_new_poly_sclae_0.dot(theta_sklearn)\n",
        "\n",
        "#Plot models\n",
        "plt.plot(X_new, y_newbig_theta_best, \"g\", linewidth=1,  label='Scikit-Learn Ridge Regression')\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.legend(loc=4);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZo-UaroDWzk"
      },
      "source": [
        "### 1.4. MAE, MSE, RMSE, $R^2$, and adjusted $R^2$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bm_-VTEX7k"
      },
      "source": [
        "It is quite common for the cost function used during training to be\n",
        "different from the performance measure used for testing. Apart from regularization, another reason they might be different is that a\n",
        "good training cost function should have optimization-friendly\n",
        "derivatives, while the performance measure used for testing should\n",
        "be as close as possible to the final objective. \n",
        "\n",
        "For example, regression model are often trained using a cost function MSE but evaluated using RMSE or $R^2$. Similarly, classifiers are often trained using a cost function such as the log loss but evaluated using precision/recall.\n",
        "\n",
        "So what are the different metric to evaluate a regression model?\n",
        "\n",
        "The MAE, MSE, RMSE, and $R^2$ metrics are mainly used to evaluate the prediction error rates and model performance of regression models.\n",
        "<br></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EWmI3Ph2RAR"
      },
      "source": [
        "#### 1.4.1- **MAE** (Mean absolute error)\n",
        "\n",
        "Represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
        "\n",
        "\n",
        "$$ MAE = \\frac{1}{m}\\sum_{i=1}^m |\\ h_{\\theta}(x^{i})- y^{i}|$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1htE2X9mK0OT"
      },
      "source": [
        "##### **Excersice MAE**\n",
        "\n",
        "Now you need to complete the `MAE` function to calculate the Mean absolute error. You need to complete the docstring information missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wkKAOSlEXHD"
      },
      "source": [
        "def MAE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        " \n",
        "\n",
        " \n",
        "    ### END CODE HERE ###\n",
        "    return MAE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj9LY1AG14I_"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJfa7g7eMgV1"
      },
      "source": [
        "y_pre=ridge_reg.predict(X_poly)\n",
        "MAE_val=MAE(y,y_pre)\n",
        "print(\"MAE=\", round(MAE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qFtpVJL2A3v"
      },
      "source": [
        "**We can also use the sklearn metric to calculate the MAE (EXPECTED OUTPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0XeBs-XOnOf"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "MAE_skl=mean_absolute_error(y, y_pre)\n",
        "print(\"sklearn MAE =\", round(MAE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ9xLeYl2csG"
      },
      "source": [
        "#### 1.4.2- **MSE** (Mean Squared Error) \n",
        "\n",
        "Represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
        "\n",
        "$$ MSE = \\frac{1}{m}\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjxSv4w91qwB"
      },
      "source": [
        "##### **Excersice MSE**\n",
        "\n",
        "Now you need to complete the `MSE`function to calculate the Mean Squared Error. You also need to complete the docstring information missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DODIQ9s_O7d1"
      },
      "source": [
        "def MSE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return MSE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L3AOjpA3KkA"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B0dwdx3PO34"
      },
      "source": [
        "MSE_val=MSE(y,y_pre)\n",
        "print(\"MSE=\", round(MSE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFjnM7NM3Ou5"
      },
      "source": [
        "**We can also use the sklearn metric to calculate the MSE (EXPECTED OUTPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41RLL7pNPxBd"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_skl=mean_squared_error(y, y_pre)\n",
        "print(\"sklearn MSE =\", round(MSE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBbnR8An2mUb"
      },
      "source": [
        "#### 1.4.3- **RMSE**(Root Mean Squared Error)\n",
        "\n",
        "\n",
        "Is the error rate by the square root of MSE.\n",
        "\n",
        "$$ RMSE = \\sqrt{MSE}= \\sqrt{ \\frac{1}{m}\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQPEeId3eZq"
      },
      "source": [
        "##### **Excersice RMSE**\n",
        "\n",
        "Now you need to complete the `RMSE`function to calculate the Root Mean Squared Error. You also need to complete the docstring information missing and cannot call the MSE function within the RMSE function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRKrVw3gQq3_"
      },
      "source": [
        "def RMSE(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return RMSE_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZqp7uiT3o-y"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKPNsPTmQ52h"
      },
      "source": [
        "RMSE_val=RMSE(y,y_pre)\n",
        "print(\"RMSE=\", round(RMSE_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBQPjNfq3q88"
      },
      "source": [
        "**We can also use the sklearn metric to calculate the RMSE (EXPECTED OUTPUT)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH-hhpcORS-s"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "RMSE_skl=sqrt(mean_squared_error(y,y_pre))\n",
        "print(\"sklearn RMSE =\", round(RMSE_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekxG2Oip2m-Q"
      },
      "source": [
        "#### 1.4.4- **$R^2$** (Coefficient of determination)\n",
        "\n",
        "\n",
        "\n",
        "Represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages (i.e., percentage of the data variability explained by the model). The higher the value is, the better the model is.\n",
        "\n",
        "$$ R^2 = 1- \\frac{\\sum_{i=1}^m\\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2}{\\sum_{i=1}^m\\left( \\bar{y} - y^{(i)}\\right)^2}$$\n",
        "\n",
        "Where $\\bar{y}$ is the mean value of $y$\n",
        "<br></br>\n",
        "\n",
        "**$R^2$-adjusted** is a modified version of $R^2$ that has been adjusted for the number of predictors in the model. The adjusted $R^2$  increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted$R^2$  can be negative, but it’s usually not.  It is always lower than the $R^2$\n",
        "\n",
        "$$ R^2-adj= 1-(1-R^2)\\left[\\frac{m-1}{m-(n+1)}\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-vaDvzJ6kxk"
      },
      "source": [
        "##### **Excersice $R^2$**\n",
        "\n",
        "Now you need to complete the `R_square`function to calculate the Coefficient of determination. You also need to complete the docstring information missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csN5zgwlSY6S"
      },
      "source": [
        "def R_square(y_true,y_pred):\n",
        "    \"\"\"\n",
        "    NEED TO COMPLETE\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array_like\n",
        "        The value at each data point. A vector of shape (m x 1).\n",
        "\n",
        "    y_pred : array_like\n",
        "        The predicted data values. A vector of shape (m x 1).\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "      NEED TO COMPLETE\n",
        "    \n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    return R_square_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8W0YAlW4GPY"
      },
      "source": [
        "Lets test you code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otU-D8crp5EX"
      },
      "source": [
        "R_square_val=R_square(y,y_pre)\n",
        "print(\"R_square=\", round(R_square_val,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2dnVy3D4LJi"
      },
      "source": [
        "**We can also use the sklearn metric to calculate the $R^2$  (EXPECTED OUTPUT)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxjID-5lqVAf"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "R_square_skl=r2_score(y, y_pre)\n",
        "print(\"sklearn R_square =\", round(R_square_skl,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "scSY7J0jR22M"
      },
      "source": [
        "# 2- Using Linear Regression for Predicting Bicycle Traffic Excersices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "deletable": true,
        "editable": true,
        "id": "UE1QvyFcR22P"
      },
      "source": [
        "As an practical example, we will look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
        "\n",
        "\n",
        "We will perform some basic feature engineering, and will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors (e.g., temperature, precipitation, and daylight hours) affect the volume of bicycle traffic.\n",
        "\n",
        "Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (We will used station ID [USW00024233](https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00024233/detail)) and we can easily use the [Pandas library](https://pandas.pydata.org/) to join the two data sources.\n",
        "\n",
        "We will perform a multivariate linear regression and a Ridge Linear Regression  to map features (i.e., weather and other information) to bicycle counts. We wil also look at the model's parameter to estimate how a change in any one of these parameters affects the number of riders on a given day.\n",
        "\n",
        "Let's start by loading the two datasets, indexing by date:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "t12toiu3R22g"
      },
      "source": [
        "#Data is already on your data folder\n",
        "counts = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)\n",
        "weather = pd.read_csv('data/BicycleWeatherdata.csv', index_col='DATE', parse_dates=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6L3ZGyYtR22p"
      },
      "source": [
        "Next we will compute the total daily bicycle traffic, and put this in its own dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "V48W3tVER22t"
      },
      "source": [
        "daily = counts.resample('d').sum()\n",
        "daily['Total'] = daily.sum(axis=1)\n",
        "daily = daily[['Total']] # remove other columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "4jxBcqoNR223"
      },
      "source": [
        "Bicycle use could vary from day to day; so let's account for this in our data by adding binary columns that indicate the day of the week. While the feature of \"day of the week\" is a categorical varaible, we can create a sereis of binary \"dummy\" variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "UDvzvDXKR227"
      },
      "source": [
        "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "for i in range(7):\n",
        "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6VxjUd5jR23E"
      },
      "source": [
        "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "3TwzgFM-R23H"
      },
      "source": [
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "cal = USFederalHolidayCalendar()\n",
        "holidays = cal.holidays('2012', '2016')\n",
        "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
        "daily['holiday'].fillna(0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LcGeqraOR23W"
      },
      "source": [
        "We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "anYH4iJMR23a"
      },
      "source": [
        "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
        "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
        "    days = (date - pd.datetime(2000, 12, 21)).days\n",
        "    m = (1. - np.tan(np.radians(latitude))\n",
        "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
        "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
        "\n",
        "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
        "daily[['daylight_hrs']].plot()\n",
        "plt.ylim(8, 17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Qix5hMrYR23j"
      },
      "source": [
        "We can also add the average temperature and total precipitation to the data.\n",
        "In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "akwgV30KR23m"
      },
      "source": [
        "# temperatures are in 1/10 deg C; convert to C\n",
        "weather['TMIN'] /= 10\n",
        "weather['TMAX'] /= 10\n",
        "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
        "\n",
        "# precip is in 1/10 mm; convert to inches\n",
        "weather['PRCP'] /= 254\n",
        "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
        "\n",
        "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zfSflTlGR23w"
      },
      "source": [
        "Finally, let's add a counter that increases from day 1, and measures how many years have passed.\n",
        "This will let us measure any observed annual increase or decrease in daily crossings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Vo-2TnzUR23y"
      },
      "source": [
        "daily['annual'] = (daily.index - daily.index[0]).days / 365."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "LHxPxGQ2R239"
      },
      "source": [
        "Now our data is in order, and we can take a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "6h4hNQr-R23_"
      },
      "source": [
        "print(\"Dataset shape:\", daily.shape)\n",
        "daily.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jRHx_10oR24J"
      },
      "source": [
        "Since we would like to test how well our predictive model would perform in new data, we need to test it performance with data that it has not seen before. To achieve this, we will first partition our dataset in a training and a test set. We will do an 70/30 partition using [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn.model_selection` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfVut9fsEWb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "daily, test = train_test_split(daily, test_size=0.3, random_state=43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si-dT8RI0Z4b"
      },
      "source": [
        "**Let's do some EDA.**\n",
        "\n",
        "You should never look at the values of your test set to avoid any bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yr3xUDq0mRe"
      },
      "source": [
        "daily.describe(include='all')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF3tptzx03Fi"
      },
      "source": [
        "daily.describe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-75JPwh09zi"
      },
      "source": [
        "As you can see from the EDA, we should probably perform feature normalization, as well as to deal with the NA's value. There are [multiple techniques to deal with NA's](https://medium.com/@george.drakos62/handling-missing-values-in-machine-learning-part-1-dda69d4f88ca), but for now we can just remove any row that has an NA value. We should also do the same for our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2PINXx7AR24M"
      },
      "source": [
        "# Drop any rows with null values\n",
        "daily.dropna(axis=0, how='any', inplace=True)\n",
        "test.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'holiday',\n",
        "                'daylight_hrs', 'PRCP', 'dry day', 'Temp (C)', 'annual']\n",
        "X = daily[column_names]\n",
        "y = daily['Total']\n",
        "\n",
        "X_test = test[column_names]\n",
        "y_test = test['Total']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qJdP4DoDR24U"
      },
      "source": [
        "Now can perform some Linear Regression and see how well were are able to predicted bicycle traffic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvVXpz8H6lAH"
      },
      "source": [
        "### 2.1- Multivariate Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "M4HyyZybR24X"
      },
      "source": [
        "#Fit Multivariate Linear Regression Model\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X, y)\n",
        "\n",
        "#Calculate the model's parameters error\n",
        "np.random.seed(1)\n",
        "err = np.std([model.fit(*resample(X, y)).coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(model.coef_, index=X.columns)\n",
        "print(pd.DataFrame({'effect': params.round(0),\n",
        "                    'error': err.round(0)}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaM-agDB59ON"
      },
      "source": [
        "\n",
        "Here we are looking a the  model's parameters with a measuremnt of their uncertainty. We computed these uncertainties by using bootstrap resamplings of the data. However, there are some other statistical ways of doing this. (see Trevor Hastie, Robert Tibshirani and Jerome Friedman (2009). Elements of Statistical Learning. Chapter 3)\n",
        "\n",
        "By lookin at the model's parameter, we first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays.\n",
        "\n",
        "We see that for each additional hour of daylight, 240 ± 22 more people choose to ride; a temperature increase of one degree Celsius encourages 133 ± 9 people to grab their bicycle; and each inch of precipitation means 1362 ± 158 more people leave their bike at home.\n",
        "\n",
        "Once all these effects are accounted for, we see a modest increase of 78 ± 41 new daily riders each year.\n",
        "\n",
        "We can now calculate some performance metrics and visually inspect how well our Linear Regression model did on the training set and testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up_tS2fG5-Ag"
      },
      "source": [
        "RMSE_training_LR=np.sqrt(mean_squared_error(y, model.predict(X)))\n",
        "print(\"RMSE from Training=\" + str(round(RMSE_training_LR, 4)))\n",
        "\n",
        "daily['predicted'] = model.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9lNN54-6HzA"
      },
      "source": [
        "test['predicted'] = model.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);\n",
        "\n",
        "RMSE_test_LR=np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
        "print(\"RMSE from Test=\" + str(round(RMSE_test_LR,4)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4cCKSdB6PmG"
      },
      "source": [
        "It is evident that we have missed some key features, especially during the summer time.\n",
        "Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).\n",
        "Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:\n",
        "\n",
        "Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n",
        "\n",
        "Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days).\n",
        "\n",
        "These are all potentially interesting effects, and you now have the tools to begin exploring them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i7M6gQcoMDj"
      },
      "source": [
        "### 2.2- Ridge Linear Regression Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V35cgHyQ_RQZ"
      },
      "source": [
        "Now is your time to implement a Ridge Linear Regression using skelearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (see our previoues notebook for example code, like Regularized_Linear_Regression.ipynb or Overfitting_Underfitting.ipynb notebook)\n",
        "\n",
        "You can perform some additional feature engineering. BUT ONLY LOOK AT THE PERFORMANCE OF YOUR MODEL ON THE TEST SET ONCE YOU MODEL IS READY (i.e., do not change your model after looking at the test set performance since this will introduce bias)\n",
        "\n",
        "The name of the pipepline should be `ridge_regression_pipeline`\n",
        "and the of the Regression step should be `Ridge_lin_reg`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='red'>WARNING: </font> The dataset already has 14 features, so increasing the degree of polynomial will rapidly increase the number of features fitted which will slow the process significantly (e.g., 14 features of degree 2= 135 features, degree 3= 815, degree 4=3, 875). Hence, start simple and check the performance on the training dataset. "
      ],
      "metadata": {
        "id": "MEQwuStkC1D9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJSXWsXHnpwY"
      },
      "source": [
        "### START CODE HERE ### (≈ 9 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the model's parameters error\n",
        "np.random.seed(1)\n",
        "err = np.std([ridge_regression_pipeline.fit(*resample(X, y)).named_steps.Ridge_lin_reg.coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(ridge_regression_pipeline.named_steps.Ridge_lin_reg.coef_, \n",
        "                   index=ridge_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\n",
        "pd.DataFrame({'effect': params.round(0),'error': err.round(0)})"
      ],
      "metadata": {
        "id": "Vs96YVLv33Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkZpyQypCPXR"
      },
      "source": [
        "#Calculate model RMSE\n",
        "RMSE_training_Ridge=np.sqrt(mean_squared_error(y, ridge_regression_pipeline.predict(X)))\n",
        "print(\"RMSE Ridge LR from Training=\" + str(round(RMSE_training_Ridge,4)))\n",
        "\n",
        "#Plot predicted values \n",
        "daily['predicted'] = ridge_regression_pipeline.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5wpumSe-GrS"
      },
      "source": [
        "RMSE_test_Ridge=np.sqrt(mean_squared_error(y_test, ridge_regression_pipeline.predict(X_test)))\n",
        "print(\"RMSE Ridge LR from Test=\" + str(round(RMSE_test_Ridge,4)))\n",
        "\n",
        "test['predicted'] = ridge_regression_pipeline.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy54ucAZ-llZ"
      },
      "source": [
        "### 2.3- Lasso Linear Regression Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM5LUl5zE5m4"
      },
      "source": [
        "Now is your time to implement a Lasso Linear Regression using skelearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (see Day9_Regularized_Linear_Regression.ipynb notebook)\n",
        "\n",
        "You can perform some additional feature engineering. BUT ONLY LOOK AT THE PERFORMANCE OF YOUR MODEL ON THE TEST SET ONCE YOU MODEL IS READY (i.e., do not change your model after looking at the test set performance since this will introduce bias).\n",
        "\n",
        "The name of the pipepline should be `lasso_regression_pipeline` and the name of your regression step should be `Lasso_lin_reg`\n",
        "(due to the issues of converganve, Lasso might take longer than Ridge LR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzKZvZMI42Cy"
      },
      "source": [
        "### START CODE HERE ### (≈ 9 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "err = np.std([lasso_regression_pipeline.fit(*resample(X, y)).named_steps.Lasso_lin_reg.coef_\n",
        "              for i in range(1000)], 0)\n",
        "\n",
        "params = pd.Series(lasso_regression_pipeline.named_steps.Lasso_lin_reg.coef_, \n",
        "                   index=lasso_regression_pipeline.named_steps.poly_features.get_feature_names(X.columns))\n",
        "table=pd.DataFrame({'effect': params.round(0),'error': err.round(0)})\n",
        "table"
      ],
      "metadata": {
        "id": "01jDi4vD4MCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgrRcTMU-C3x"
      },
      "source": [
        "Lets get the features with non-zero weights/coefficients "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UBpm2Md8hJN"
      },
      "source": [
        "table[ table['effect']!=0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mCpWYxpERXf"
      },
      "source": [
        "RMSE_training_Lasso=np.sqrt(mean_squared_error(y, lasso_regression_pipeline.predict(X)))\n",
        "print(\"RMSE from Training=\" + str(round(RMSE_training_Lasso,4)))\n",
        "\n",
        "daily['predicted'] = lasso_regression_pipeline.predict(X)\n",
        "daily[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE4Y_RQjDCqH"
      },
      "source": [
        "\n",
        "RMSE_test_Lasso=np.sqrt(mean_squared_error(y_test, lasso_regression_pipeline.predict(X_test)))\n",
        "print(\"RMSE from Test=\" + str(round(RMSE_test_Lasso,4)))\n",
        "\n",
        "test['predicted'] = lasso_regression_pipeline.predict(X_test)\n",
        "test[['Total', 'predicted']].plot(alpha=0.5);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubozIj4KGUXO"
      },
      "source": [
        "As you have experienced, we can do a lot of different things to make our model perform better (change the regularization term, add or remove features,….). But how can we make this process more efficient? \n",
        "\n",
        "\n",
        "Well, we could use a process to identify the hyperparameter that will create the model that best fit the data, similar to the training process that helps us identify the model’ parameter that best fit the data (we will talk more about hyperparameter optimization/tuning later on the class) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyuBIfV4O2l6"
      },
      "source": [
        "# 3- Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN5PmBs5PjbM"
      },
      "source": [
        "Remember about the sigmoid function or logistic function we will use for our hypothesis. We have that:\n",
        "\n",
        "$$h_\\theta(X)=g(X\\theta)  $$\n",
        "\n",
        "$$g(X\\theta)= \\frac{1}{1+e^{-X\\theta}}$$\n",
        "\n",
        "$$OR$$\n",
        "\n",
        "$$g(z)= \\frac{1}{1+e^{-z}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poqM6jRQO2FK"
      },
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$g(z) = \\frac{1}{1 + e^{-z}}$\")\n",
        "plt.xlabel(\"z\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLqaht8cPzq8"
      },
      "source": [
        " Let's look at the Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JfYXsfOP15a"
      },
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWs7tFRrP3mq"
      },
      "source": [
        "print(iris.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2bsY_uiP3On"
      },
      "source": [
        "## 3.1- Logistic Regression for Binary Classification (y=1 or 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHKqqTgWQC48"
      },
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5d1o48PQGIk"
      },
      "source": [
        "The `LogisticRegression` functions from skeearn has a lot o different parameters we can change. To learn more about them look [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaoOnl3cQD-k"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFTVvhDPQJhc"
      },
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXp7ruECQMlE"
      },
      "source": [
        "decision_boundary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE-cOn8_QPHD"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)\n",
        "\n",
        "log_reg = LogisticRegression(C=10**10, random_state=42)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qstt_ecjby0G"
      },
      "source": [
        "The model's coefficients can provide the basis for a crude feature importance score. This assumes that the input variables have the same scale or have been scaled prior to fitting a model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vPyCUzbe1v"
      },
      "source": [
        "log_reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyKgbqFxQSnP",
        "cellView": "form"
      },
      "source": [
        "\n",
        "Degree_of_the_Polynomial_Model=20 #@param {type:\"integer\", min:1, max:14, step:1}\n",
        "\n",
        "\n",
        "polybig_features = PolynomialFeatures(degree=Degree_of_the_Polynomial_Model, include_bias=False)\n",
        "std_scaler = StandardScaler()\n",
        "log_reg = LogisticRegression(C=10**10, random_state=42,max_iter=10000)\n",
        "\n",
        "\n",
        "Poly_log_regression = Pipeline([\n",
        "        (\"poly_features\", polybig_features),\n",
        "        (\"std_scaler\", std_scaler),\n",
        "        (\"log_reg\", log_reg),\n",
        "    ])\n",
        "\n",
        "\n",
        "Poly_log_regression.fit(X,y)\n",
        "\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(2.9, 7, 500).reshape(-1, 1),\n",
        "        np.linspace(0.8, 2.7, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = Poly_log_regression.predict_proba(X_new)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"g^\")\n",
        "\n",
        "zz = y_proba[:, 1].reshape(x0.shape)\n",
        "contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)\n",
        "\n",
        "\n",
        "left_right = np.array([2.9, 7])\n",
        "boundary = -(log_reg.coef_[0][0] * left_right + log_reg.intercept_[0]) / log_reg.coef_[0][1]\n",
        "\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "#plt.plot(left_right, boundary, \"k--\", linewidth=3)\n",
        "plt.text(3.5, 1.5, \"Not Iris virginica\", fontsize=14, color=\"b\", ha=\"center\")\n",
        "plt.text(6.5, 2.3, \"Iris virginica\", fontsize=14, color=\"g\", ha=\"center\")\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.axis([2.9, 7, 0.8, 2.7])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gCLhcTNRTna"
      },
      "source": [
        "## 3.2- Softmax Regression for Multiclass Classification (k>2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye8NeVAtRsek"
      },
      "source": [
        "Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) is a generalization of logistic regression that we can use for multi-class classification (under the assumption that the classes are mutually exclusive). We will use the Iris dataset again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgwVjSVwSODB"
      },
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRGXGOj2Rwll"
      },
      "source": [
        "Let's set the random seed so the output of this exercise solution is reproducible and load the iris dataset again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOa0laffR60y"
      },
      "source": [
        "np.random.seed(2042)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKMaQoUKR6Mw"
      },
      "source": [
        "Also, lets start splitting our dataset. The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn's `train_test_split()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnKtkIWpSCZu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoErhYoZSwWE"
      },
      "source": [
        "Lets look at our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgTvCov7QuVV"
      },
      "source": [
        "print(\"Shape of  X training set: \" + str(X_train.shape))\n",
        "print(\"Shape of X test set: \" + str(X_test.shape))\n",
        "\n",
        "print(\"Shape of  y training set: \" + str(y_train.shape))\n",
        "print(\"Shape of y test set: \" + str(y_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s29xbNDXTPoJ"
      },
      "source": [
        "##### **Excersice**\n",
        "\n",
        "Now train a Softmax Linear Regression model. You migth want to look at the documentation of [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZWxIxygRhm5"
      },
      "source": [
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n",
        "coef_sogmax=softmax_reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iygYh6ZSTrob"
      },
      "source": [
        "Now let’s generate the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) and calculate some performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWavfU5JTrI7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# get the names of the classes\n",
        "iris = datasets.load_iris()\n",
        "class_names = iris.target_names\n",
        "\n",
        "y_pred=softmax_reg.predict(X_test)\n",
        "print(classification_report(y_test,  y_pred, target_names=class_names))\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
        "                  (\"Normalized confusion matrix\", 'true')]\n",
        "for title, normalize in titles_options:\n",
        "    disp = plot_confusion_matrix(softmax_reg, X_test, y_test,\n",
        "                                 display_labels=class_names,\n",
        "                                 cmap=plt.cm.Blues,\n",
        "                                 normalize=normalize)\n",
        "    disp.ax_.set_title(title)\n",
        "\n",
        "    print(title)\n",
        "    print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7DqGAMXXH_R"
      },
      "source": [
        "Looking at the confusion matrix and at our performance metric, it looks like the SoftMax regression model did an excellent job a classifying the instances from the test set (i.e., the accuracy of 100%). This is because the iris dataset is not very “complex”, if you do some EDA on the data you will find out that it is very easy to differentiate the classes based on  just Pedal width and Pedal length ( see  the image below. )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE_mK0nmQwtU"
      },
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X_test[y_test==2, 0], X_test[y_test==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X_test[y_test==1, 0], X_test[y_test==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X_test[y_test==0, 0], X_test[y_test==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Plf5hnWjaY"
      },
      "source": [
        "# 4- Using Logistic Regression for Predicting Wine Quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBSqQGF_dMma"
      },
      "source": [
        "For this exercise, you need to train a SoftMax regression model to predict wine quality group.  You need to:\n",
        "\n",
        "1. Split the dataset into a training and test set using a 70/30 partition\n",
        "2. Training a SoftMax model on the training dataset\n",
        "3. Test your model on your test set\n",
        "4. Show performance metric (i.e., accuracy, f1- score)\n",
        "5. Plot a confusion matrix with eh test set. \n",
        "\n",
        "I will upload the full dataset in X and y variables (as we did with the Iris dataset), plus the class names. Be advised that all the code you need to complete this, is already elsewhere on this notebook.\n",
        "\n",
        "#### <font color='yellow'>HINT: </font> The code need to complete this was already provided on the example above. However, you cannot just copy and paste, you need to understand the basic of the code and identify how you need to modified to accept different inputs. Also, be advise you about the difference between the whole dataset, the training and the test set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UwLSNbzWyIh"
      },
      "source": [
        "np.random.seed(2042)\n",
        "from sklearn import datasets\n",
        "wine = datasets.load_wine()\n",
        "\n",
        "X = wine[\"data\"]\n",
        "y = wine[\"target\"].astype(np.int)  \n",
        "class_names = wine.target_names\n",
        "\n",
        "list(wine.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeQmeBlJW4QB"
      },
      "source": [
        "print(wine.DESCR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNE3tg07XDQh"
      },
      "source": [
        "### START CODE HERE ### (≈ 25 lines of code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### END CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5d-t8sX4bD"
      },
      "source": [
        "###### **DO NOT DELETE NOR MODIFY THESE CODE CELLS**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "try:\n",
        "    X_norm_sklearn\n",
        "except:\n",
        "    X_norm_sklearn=None\n",
        "try:\n",
        "    X_norm\n",
        "except:\n",
        "    X_norm=None\n",
        "try:\n",
        "    theta_best\n",
        "except:\n",
        "    theta_best=None\n",
        "try:\n",
        "    theta_cost\n",
        "except:\n",
        "    theta_cost=None\n",
        "try:\n",
        "    MAE_val\n",
        "except:\n",
        "    MAE_val=None\n",
        "try:\n",
        "    MAE_skl\n",
        "except:\n",
        "    MAE_skl=None\n",
        "try:\n",
        "    MSE_val\n",
        "except:\n",
        "    MSE_val=None\n",
        "\n",
        "try:\n",
        "    MSE_skl\n",
        "except:\n",
        "    MSE_skl=None\n",
        "try:\n",
        "    RMSE_val\n",
        "except:\n",
        "    RMSE_val=None\n",
        "try:\n",
        "    RMSE_skl\n",
        "except:\n",
        "    RMSE_skl=None\n",
        "try:\n",
        "    R_square_val\n",
        "except:\n",
        "    R_square_val=None\n",
        "\n",
        "try:\n",
        "    R_square_skl\n",
        "except:\n",
        "    R_square_skl=None\n",
        "\n",
        "try:\n",
        "    ridge_regression_pipeline\n",
        "except:\n",
        "    ridge_regression_pipeline=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_training_Ridge\n",
        "except:\n",
        "    RMSE_training_Ridge=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_test_Ridge\n",
        "except:\n",
        "    RMSE_test_Ridge=None\n",
        "\n",
        "try:\n",
        "    RMSE_training_Lasso\n",
        "except:\n",
        "    RMSE_training_Lasso=None\n",
        "\n",
        "\n",
        "try:\n",
        "    RMSE_test_Lasso\n",
        "except:\n",
        "    RMSE_tRMSE_test_Lassoest_Ridge=None\n",
        "\n",
        "\n",
        "try:\n",
        "    lasso_regression_pipeline\n",
        "except:\n",
        "    lasso_regression_pipeline=None\n",
        "\n",
        "try:\n",
        "    coef_sogmax\n",
        "except:\n",
        "    coef_sogmax=None\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from GRADING_MRLR import GRADING\n",
        "\n",
        "GRADING(X_norm_sklearn,\n",
        "        X_norm,theta_best,\n",
        "        theta_cost,\n",
        "        MAE_val,\n",
        "        MAE_skl,\n",
        "        MSE_val,\n",
        "        MSE_skl,\n",
        "        RMSE_val,\n",
        "        RMSE_skl,\n",
        "        R_square_val,\n",
        "        R_square_skl,\n",
        "        RMSE_training_LR,\n",
        "        RMSE_test_LR,\n",
        "        ridge_regression_pipeline,\n",
        "        RMSE_training_Ridge,\n",
        "        RMSE_test_Ridge,\n",
        "        RMSE_training_Lasso,\n",
        "        RMSE_test_Lasso,\n",
        "        lasso_regression_pipeline,\n",
        "        coef_sogmax)"
      ],
      "metadata": {
        "id": "Q3bLKilVFmNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}